<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weize Li's Homepage</title>

  <meta name="author" content="Weize Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Weize Li
                  </p>
                  <p>I'm
                    a Research Engineer at <a
                      href="https://technode.com/2025/07/09/embodied-ai-startup-tars-completes-122-million-angel-funding-round-report/"
                      target="_blank">TARS Robotics</a>, advised by <a
                      href="https://wenchaoding.github.io/personal/index.html" target="_blank">Prof. Wenchao
                      Ding</a> and <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm" target="_blank">Prof.
                      Yilun Chen</a>.
                    I work on full stack robot learning (manipulation), touching whole circle of the data collection,
                    simulation, model/policy training, and inference deployment.
                    <!-- Previously, I was fortunate to work closely with <a href="https://www.xxlong.site/"
                      target="_blank"><u>Prof. Xiaoxiao Long</u></a> from <a href="https://www.nju.edu.cn/en/"
                      target="_blank"><u>NJU</u></a>, <a href="https://ece.hkust.edu.hk/pingtan"
                      target="_blank"><u>Prof. Ping Tan</u></a> from <a href="https://hkust.edu.hk/"
                      target="_blank"><u>HKUST</u></a>, and <a href="https://sites.google.com/view/fromandto"
                      target="_blank"><u>Prof. Hao Zhao</u></a> from <a href="https://www.discover-lab.com/"
                      target="_blank"><u>DISCOVER Lab</u></a>. -->
                    Previously, I spent several gap years as an Research Assistant/Intern at <a
                      href="https://www.discover-lab.com/" target="_blank">AIR, Tsinghua University</a> and <a
                      href="https://hkust.edu.hk/" target="_blank">HKUST</a>, working with exceptional mentors to
                    dive into 3D vision and graphics.
                    I was a visiting student at the <a href="http://english.ia.cas.cn/" target="_blank">Institute of
                      Automation, Chinese Academy of Sciences</a> in my senior year.

                    <!-- <p>I am always open to collaboration and discussion ‚Äî feel free to drop me an email if you are
                    interested in my research.</p> -->

                  </p>
                  <p>

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:liweize0224@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=CyPiUucAAAAJ&hl=en">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/EricLee0224">Github</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/weize-li-356459250/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://x.com/WeizeLi24">X</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:21%;max-width:21%">
                  <a href="assets/weize_animation.jpg"><img style="width:100%;max-width:100%;object-fit: cover;"
                      alt="profile photo" src="assets/weize_animation.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Experience</h2>
                  <br>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                    <tbody>
                      <tr>
                        <td style="width:12%;vertical-align:middle;padding-right:15px;">
                          <img src="assets/TARS_BLK_LOGO.png" alt="TARS" style="width:100%;height:auto;">
                        </td>
                        <td style="vertical-align:top;">
                          <p style="margin:0;"><strong>TARS Robotics | AWE</strong></p>
                          <p style="margin:0;">Research Engineer</p>
                          <p style="margin:0;">
                            Mentors:
                            <a href="https://wenchaoding.github.io/personal/index.html">Prof. Wenchao Ding</a>,
                            <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Prof. Yilun Chen</a>
                          </p>
                        </td>
                        <td style="width:20%; text-align:right; vertical-align:top; white-space:nowrap;">
                          Mar 2025 ‚Äì Present
                        </td>
                      </tr>
                    </tbody>
                  </table>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                    <tbody>
                      <tr>
                        <td style="width:12%;vertical-align:middle;padding-right:15px;">
                          <img src="assets/AIRIC_logo.jpg" alt="AIRIC" style="width:100%;height:auto;">
                        </td>
                        <td style="vertical-align:top;">
                          <p style="margin:0;"><strong>Tsinghua University | AIR Innovation Center</strong></p>
                          <p style="margin:0;">Research Assistant</p>
                          <p style="margin:0;">
                            Mentor:
                            <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Prof. Yilun Chen</a>
                          </p>
                        </td>
                        <td style="width:20%; text-align:right; vertical-align:top; white-space:nowrap;">
                          Jan 2025 ‚Äì May 2025
                        </td>
                      </tr>
                    </tbody>
                  </table>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                    <tbody>
                      <tr>
                        <td style="width:12%;vertical-align:middle;padding-right:15px;">
                          <img src="assets/HKUST_Logo.png" alt="HKUST" style="width:120%;height:auto;">
                        </td>
                        <td style="vertical-align:top;">
                          <p style="margin:0;"><strong>Hong Kong University of Science and Technology (HKUST) |
                              LightIllusions</strong></p>
                          <p style="margin:0;">Research Intern</p>
                          <p style="margin:0;">
                            Mentors:
                            <a href="https://www.xxlong.site/">Prof. Xiao-xiao Long</a>,
                            <a href="https://ece.hkust.edu.hk/pingtan">Prof. Ping Tan</a>
                          </p>
                        </td>
                        <td style="width:20%; text-align:right; vertical-align:top; white-space:nowrap;">
                          Apr 2024 - Oct 2024
                        </td>
                      </tr>
                    </tbody>
                  </table>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;">
                    <tbody>
                      <tr>
                        <td style="width:12%;vertical-align:middle;padding-right:15px;">
                          <img src="assets/AIR_logo.png" alt="AIR" style="width:100%;height:auto;">
                        </td>
                        <td style="vertical-align:top;">
                          <p style="margin:0;"><strong>Tsinghua University | AIR</strong></p>
                          <p style="margin:0;">Research Intern</p>
                          <p style="margin:0;">
                            Mentors:
                            <a href="https://sites.google.com/view/fromandto">Prof. Hao Zhao</a>,
                            <a href="https://pku-hmi-lab.github.io/HMI-Web/leader.html">Prof. Shanghang Zhang</a>,
                            <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Prof. Yilun Chen</a>
                          </p>
                        </td>
                        <td style="width:20%; text-align:right; vertical-align:top; white-space:nowrap;">
                          Aug 2022 - Dec 2024
                        </td>

                      </tr>
                    </tbody>
                  </table>

                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I am broadly interested in the intersection of robotics, 3D vision and multimodal learning, with
                    the long-term goal of building embodied intelligent systems capable of human-level manipulation.
                    Currently, I am focusing on foundation model for robotics, bimanual manipulation and human-centric
                    cross-embodiment transfer.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr>
                <td style="padding:30px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;top: 10px; background-color: white;"
                      src='assets/publications/pokevla.png' height="160" width="180">
                  </div>
                </td>


                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">Pok√©VLA: Empowering Pocket-Sized Vision-Language-Action Model with
                      Comprehensive World Knowledge Guidance</span>
                  </a>
                  <br>

                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=en">Yupeng Zheng</a>,
                  <a href=" ">Xiang Li</a>,
                  <a href="https://scholar.google.com/citations?user=W-msX90AAAAJ&hl=en">Songen Gu</a>,
                  <a href=" ">Yuhang Zheng</a>,
                  <a href=" ">Shuai Tian</a>,
                  <strong>Weize Li</strong>,
                  <a href=" ">Linbo Wang</a>,
                  <a href=" ">Senyu Fei</a>,
                  <a href=" ">Pengfei Li</a>,
                  <a href=" ">YinFeng Gao</a>,
                  <a href=" ">Zebin Xing</a>,
                  <a href=" ">Qichao Zhang</a>,
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Yilun Chen</a>,
                  <a href="https://wenchaoding.github.io/">Wenchao Ding</a>,
                  <a href=" ">Haoran Li</a>.
                  <br>

                  <em>In Submission</em>, 2025
                  <br>

                  <a href="https://arxiv.org/abs/">arXiv</a>

                  <p></p>

                  <!-- <p>
                    Pok√©VLA is a lightweight Vision-Language-Action model that injects rich vision-language
                    understanding into robot manipulation via a two-stage training framework, achieving state-of-the-art
                    performance and robustness in both benchmarks and real-world settings.
                  </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:30px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;top: 10px; background-color: white;"
                      src='assets/publications/wiyh_teaser.png' height="160" width="180">
                  </div>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">World in Your Hands: A Large-Scale and Open-source Ecosystem for Learning
                      Human-centric Manipulation in the Wild</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=en">Yupeng Zheng*</a>,
                  Jichao Peng*,
                  <strong>Weize Li</strong>,
                  <a href="https://github.com/xbillowy">Yuhang Zheng</a>,
                  <a href="https://yvanyin.net/">Xiang Li</a>,
                  Yujie Jin, Julong Wei, Guanhua Zhang, Ruiling Zheng, Ming Cao,
                  Songen Gu, Zhenhong Zou, Kaige Li, Ke Wu, Mingmin Yang, JiahaoLiu,
                  Pengfei Li, Hengjie Si, Feiyu Zhu, Wang Fu, Likun Wang, Ruiwen Yao,
                  <a href="https://zjru.github.io/">Jieru Zhao</a>,
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Yilun Chen</a>,
                  <a href="https://wenchaoding.github.io/">Wenchao Ding</a>.
                  <br>

                  <em>In Submission</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/">arXiv</a>
                  <p></p>
                  <!-- <p>
                    World in Your Hands is a large-scale, 1,000-hour human-centric manipulation dataset and benchmark
                    suite that greatly improves the understanding of bimanual dexterous manipulation and interaction.
                  </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;top: 10px; background-color: white;"
                      src='assets/publications/litevggt_teaser.png' height="160" width="180">
                  </div>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token
                      Merging</span>
                  </a>
                  <br>

                  <a href="https://garlicba.github.io/">Zhijian Shu</a>,
                  <a href="https://clinplayer.github.io/">Cheng Lin</a>,
                  <a href="https://github.com/xbillowy">Tao Xie</a>,
                  <a href="https://yvanyin.net/">Wei Yin</a>,
                  <a href="https://github.com/fanerlight">Ben Li</a>,
                  <a href="https://openreview.net/profile?id=~Zhiyuan_Pu1">Zhiyuan Pu</a>,
                  <strong>Weize Li</strong>,
                  <a href="https://yoyo000.github.io/">Yao Yao</a>,
                  <a href="https://openreview.net/profile?id=~Xun_Cao1">Xun Cao</a>,
                  <a href="https://xy-guo.github.io/">Xiaoyang Guo</a>
                  <a href="https://xxlong0.github.io/">Xiao-xiao Long</a>.
                  <br>

                  <em>In Submission</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/">arXiv</a>
                  <p></p>

                  <!-- <p>
                    LiteVGGT speeds up VGGT by up to 10√ó with far lower memory cost through geometry-aware cached token
                    merging, enabling efficient 3D reconstruction on 1000-image scenes with minimal accuracy loss.
                  </p> -->
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;top: 10px; background-color: white;"
                      src='assets/publications/uniart_pipeline.png' height="125" width="190">
                  </div>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">UniArt: Unified 3D Representation for Generating 3D Articulated Objects
                      with Open-Set Articulation</span>
                  </a>
                  <br>

                  <a href="https://scholar.google.com/citations?user=uUd5v2cAAAAJ&hl=en">Bu Jin</a>,
                  <strong>Weize Li</strong>,
                  <a href="https://scholar.google.com/citations?user=W-msX90AAAAJ&hl=en">Songen Gu</a>,
                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=en">Yupeng Zheng</a>,
                  <a href="https://scholar.google.com/citations?user=Wn2Aic0AAAAJ&hl=en">Yuhang Zheng</a>,
                  <a>Zhengyi Zhou</a>,
                  <a href="https://yoyo000.github.io/">Yao Yao</a>.

                  <br>
                  <em>In Submission</em>, 2025
                  <br>

                  <a href="https://arxiv.org/abs/">arXiv</a>

                  <p></p>

                  <!-- <p>
                    UniArt generates fully articulated 3D objects from a single image using a unified diffusion model
                    that jointly learns geometry, appearance, parts, and joint motions.
                  </p> -->
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;top: 10px; background-color: white;"
                      src='assets/publications/vistabot_pipeline.png' height="125" width="190">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">VistaBot: View-Robust Robot Manipulation via Spatiotemporal-Aware View
                      Synthesis</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=W-msX90AAAAJ&hl=en">Songen Gu</a>,
                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=en">Yupeng Zheng</a>,
                  <a href="https://scholar.google.com/citations?user=Wn2Aic0AAAAJ&hl=en">Yuhang Zheng</a>,
                  <strong>Weize Li</strong>,
                  <a>Yating Feng</a>,
                  <a href="https://getterupper.github.io/">Xiang Li</a>,
                  <a href="https://philipflyg.github.io/">Pengfei Li</a>,
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Yilun Chen</a>,
                  <a href="https://wenchaoding.github.io/">Wenchao Ding</a>.
                  <br>
                  <em>In Submission</em>, 2025
                  <br>
                  <a href="http://arxiv.org/abs/">arXiv</a>
                  <p></p>
                  <!-- <p>
                    VistaBot makes robot manipulation robust to camera viewpoint changes by combining 4D geometry with
                    video diffusion models‚Äîno camera calibration needed.
                  </p> -->
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;top: 15px;" src='assets/publications/wbcd_final.svg' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2401.01391">
                    <span class="papertitle">Taming VR Teleoperation and Learning from Demonstration for Multi-Task
                      Bimanual Table Service Manipulation</span>
                  </a>
                  <br>
                  <strong>Weize Li</strong>,
                  <a href="https://absrat.com/">Zhengxiao Han</a>,
                  <a href="https://davidlxu.github.io/">Lixin Xu</a>,
                  <a href="https://changerc77.github.io/">Xiangyu Chen</a>,
                  <a href="https://harrisonbounds.github.io/">Harrison Bounds</a>,
                  <a>Chenrui Zhang</a>,
                  <a href="https://scholar.google.com/citations?user=RYKMFp4AAAAJ&hl=en">Yifan Xu</a>.
                  <br>
                  <em>Technical Report</em>, 2025
                  <br>
                  <em>IEEE ICRA WBCD 2025 Challenge</em>, <a style="color: red;">1st Place Prize in Table Service
                    Track</a>
                  <br>
                  <a href="https://arxiv.org/abs/2508.14542">arXiv</a>
                  <p></p>
                  <!-- <p>
                    We won the ICRA 2025 WBCD Table Service Track with a champion solution that combines VR
                    teleoperation and Learning from Demonstrations for efficient and reliable bimanual robot
                    manipulation.
                  </p> -->
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position: relative; top: 12px" src='./assets/publications/robogem_teaser.svg'
                      height="100" width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <span class="papertitle">RoboGEM: Learning Language-guided Robotic Manipulation via Generalizable
                      and Efficient Feature Distillation</span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/chunzheng-wang-06a024366/">Chunzheng Wang</a>,
                  <a href="https://mrsecant.github.io/">Yuhang Zheng</a>,
                  <a href="https://changerc77.github.io/">Xiangyu Chen</a>,
                  <strong>Weize Li</strong>,
                  <a href="https://openreview.net/profile?id=%7ESongen_Gu2">Songen Gu</a>,
                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=zh-CN">Yupeng Zheng</a>.
                  <br>

                  <em>ACM International Conference on Multimedia (ACM MM)</em>, 2025
                  <br>
                  <em>RoboSoft'25 Workshop, <a style="color: red;">Best Paper Award</a>
                    <br>
                    <a href="https://arxiv.org/abs/">arXiv</a>
                    <br>
                    <p></p>
                    <!-- <p>
                      RoboGEM is a generalizable and efficient 3D representation that empowers robots to perform diverse
                      manipulation tasks with speed and robustness.
                    </p> -->
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position:relative;" src='assets/publications/ppmethod_new.png' width="180">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/">
                    <span class="papertitle">PosePilot: Steering Camera Pose for Generative World Models with
                      Self-supervised Depth</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=uUd5v2cAAAAJ&hl=en">Bu Jin<sup>*</sup></a>,
                  <strong>Weize Li<sup>*</sup></strong>,
                  <a>Baihan Yang</a>,
                  <a>Zhenxin Zhu</a>,
                  <a>Junpeng Jiang</a>,
                  <a href="https://c7w.tech/about/">Huan-ang Gao</a>,
                  <a>Haiyang Sun</a>,
                  <a>Kun Zhan</a>,
                  <a>Hengtong Hu</a>,
                  <a>Xueyang Zhang</a>,
                  <a href="https://scholar.google.com/citations?user=Z_QY_VwAAAAJ">Peng Jia</a>,
                  <a href="https://sites.google.com/view/fromandto">Hao Zhao</a>.
                  <br>
                  <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2025
                  <br>
                  <!-- <a href=" ">project page</a>
          / -->
                  <a href=" ">arXiv</a>
                  <!-- /
          <a href=" ">code</a> -->
                  <p></p>
                  <!-- <p>
                    PosePilot is a lightweight framework that enhances camera pose controllability in generative world
                    models, enabling precise, consistent, and adaptable viewpoint synthesis for autonomous driving and
                    beyond.
                  </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position: relative; left: 6px" src='./assets/publications/survey_teaser_new2.png'
                      height="140" width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://cong-yi.github.io/projects/dualmeshudf/">
                    <span class="papertitle">Radiance Field-Based 3D Editing: A Survey</span>
                  </a>
                  <br>
                  <strong>Weize Li<sup>*</sup></strong>,
                  <a href="https://tianshukuai.github.io/">Tianshu Kuai<sup>*</sup></a>,
                  <a href="https://c7w.tech/about/">Huan-ang Gao</a>,
                  <a href="https://xiangyueliu.github.io/">Xiangyue Liu</a>,
                  <a href="https://scholar.google.com/citations?user=Wn2Aic0AAAAJ&hl=en">Yuhang Zheng</a>,
                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=en">Yupeng Zheng</a>, etc.
                  <br>
                  <em>In Submission</em>, 2025
                  <br>
                  <!-- <a href=" ">project page</a>
                  / -->
                  <a href="https://arxiv.org/abs/">arXiv</a>
                  <!-- /
                  <a href=" ">code</a> -->
                  <p></p>
                  <!-- <p>
                    This survey reviews recent advances, methods, datasets, and applications in Radiance Field-based 3D
                    Editing, highlighting challenges and future directions.
                  </p> -->
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position: relative; left: 6px" src='./assets/publications/tod3cap.png' height="140"
                      width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://cong-yi.github.io/projects/dualmeshudf/">
                    <span class="papertitle">TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=uUd5v2cAAAAJ&hl=en">Bu Jin</a>,
                  <a href="https://scholar.google.com/citations?user=anGhGdYAAAAJ&hl=en">Yupeng Zheng<sup>‚Ä†</sup></a>,
                  <a href="https://github.com/Philipflyg">Pengfei Li</a>,
                  <strong>Weize Li</strong>,
                  <a href="https://scholar.google.com/citations?user=Wn2Aic0AAAAJ&hl=en">Yuhang Zheng</a>,
                  <a>Sujie Hu</a>,
                  <a href="https://liuxinyv.github.io/">Xinyu Liu</a>,
                  <a>Jinwei Zhu</a>,
                  <a href="https://scholar.google.com/citations?user=4PXGeaYAAAAJ">Zhijie Yan</a>,
                  <a>Haiyang Sun</a>,
                  <a>Kun Zhan</a>,
                  <a href="https://scholar.google.com/citations?user=Z_QY_VwAAAAJ">Peng Jia</a>,
                  <a href="https://www.xxlong.site/">Xiaoxiao Long</a>,
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1621.htm">Yilun Chen</a>,
                  <a href="https://sites.google.com/view/fromandto">Hao Zhao</a>.
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2024
                  <br>
                  <!-- <a href=" ">project page</a>
                  / -->
                  <a href="https://arxiv.org/abs/">arXiv</a>
                  <!-- /
                  <a href=" ">code</a> -->
                  <p></p>
                  <!-- <p>
                    We introduce TOD3Cap, a benchmark dataset and network for outdoor 3D dense captioning, enabling
                    accurate object localization and rich natural language descriptions from LiDAR and panoramic images.
                  </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position: relative; left: 6px" src='./assets/publications/pad.png' height="140"
                      width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://cong-yi.github.io/projects/dualmeshudf/">
                    <span class="papertitle">PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=CMYTxUEAAAAJ&hl=en/">Qiang Zhou<sup>*</sup></a>,
                  <strong>Weize Li<sup>*</sup></strong>,
                  <a href="https://jianglh-whu.github.io/">Lihan Jiang</a>,
                  <a href="https://github.com/Cross-ZBuild">Guoliang Wang</a>,
                  <a href="https://air.tsinghua.edu.cn/en/info/1046/1196.htm">Guyue Zhou</a>,
                  <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>,
                  <a href="https://sites.google.com/view/fromandto">Hao Zhao</a>.
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS) </em>, 2023
                  <br>
                  <!-- <a href=" ">project page</a>
                  / -->
                  <a href="https://arxiv.org/abs/">arXiv</a>
                  <!-- /
                  <a href=" ">code</a> -->
                  <p></p>
                  <!-- <p>
                    We introduce the MAD dataset, PAD benchmark, and OmniposeAD method
                    to tackle pose-agnostic object anomaly detection with diverse 3D
                    anomalies and standardized evaluation.
                  </p> -->
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img style="position: relative; left: 6px" src='./assets/publications/irflmdnn.png' height="140"
                      width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://cong-yi.github.io/projects/dualmeshudf/">
                    <span class="papertitle">IRFLMDNN: Hybrid Model for PMU Data Anomaly Detection and Re-filling with
                      Improved Random Forest and Levenberg Marquardt Algorithm Optimized Dynamic Neural Network</span>
                  </a>
                  <br>
                  <a href=" ">Miao Yu‚Ä†</a>,
                  <a href=" ">Chenyu Yang<sup>*</sup></a>,
                  <strong>Weize Li<sup>*</sup></strong>,
                  <a href=" ">Weijie Du</a>,
                  <a href=" ">Jinglin Li</a>.
                  <br>
                  <em>Neural Computing and Application</em>, 2023
                  <br>
                  <!-- <a href=" ">project page</a>
                  / -->
                  <a href="https://arxiv.org/abs/">arXiv</a>
                  <!-- /
                  <a href=" ">code</a> -->
                  <p></p>
                  <!-- <p>
                    This survey reviews recent advances, methods, datasets, and applications in Radiance Field-based 3D
                    Editing, highlighting challenges and future directions.
                  </p> -->
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Honors and Awards</h2>
                  <ul>
                    <li>
                      <strong>Best Paper Award, ACM MM RoboSoft'25 Workshop.</strong> | Program Committee.
                    </li>
                    <li>
                      <strong>Champion Award, IEEE ICRA 2025 WBCD Challenge (Table Services Track).</strong> | 1st WBCD
                      Challenge Organizer Committee.
                    </li>
                    <li>
                      <strong>Best Undergraduate Thesis Award - Class of 2022.</strong> | Beijing Education Commission.
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Academic Service</h2>
                  <p>
                    <strong>Reviewer:</strong>
                    <br>
                    Conference: NeurIPS'23, CVPR'24, ICRA'24, ICLR'25, IROS'25, CVPR'26.
                    <br>
                    Journal: IJCV, R-AL.
                  </p>
                  <p>
                    <strong>Organizer:</strong>
                    <br>
                    2nd What Bimanuals Can Do (WBCD) Competition, IEEE ICRA 2026.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Misc.</h2>
                  <p>
                    Outside of my research, I also enjoy photographyüì∏, fitness trainingüí™, and ball sports (such as
                    football‚öΩ,
                    basketballüèÄ, tennisüéæ, badmintonüè∏, etc.). I am also a registered refereeü™™ with the Chinese
                    Football
                    Association.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Last updated Dec. 2025.
                    Template from <a href="https://jonbarron.info/">Jon Barron.</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- Statcounter code for Personal homepage https://ericlee0224.github.io/ on
  Google Sites (new) -->
  <script type="text/javascript">
    var sc_project = 13190747;
    var sc_invisible = 1;
    var sc_security = "0caaa7fb"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/13190747/0/0caaa7fb/1/" alt="Web Analytics"
          referrerPolicy="no-referrer-when-downgrade"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>